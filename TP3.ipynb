{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2bmOkTaIPVuU2YCSYfIez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NabilNkhili/Doc_Structur-s/blob/main/TP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexer une collection de 9,804 documents sans utiliser de stop-words ni stemming.\n",
        "\n",
        "-Temps total d'indexation.\n",
        "\n",
        "-Nombre total de tokens (termes individuels) et de tokens distincts.\n",
        "\n",
        "-Longueur moyenne des tokens distincts.\n",
        "\n",
        "-Nombre total et distinct de termes après la normalisation.\n",
        "\n",
        "-Longueur moyenne des documents.-"
      ],
      "metadata": {
        "id": "YT4UCITC_32V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(content):\n",
        "    return [word.lower() for word in content.split() if word.isalpha()]\n",
        "\n",
        "def index_collection(file_path, chunk_size=1024 * 1024):\n",
        "    index = defaultdict(list)\n",
        "    total_tokens = 0\n",
        "    total_chars = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        while chunk := file.read(chunk_size):\n",
        "            for doc_id, line in enumerate(chunk.splitlines(), start=1):\n",
        "                terms = tokenize(line)\n",
        "                total_tokens += len(terms)\n",
        "                total_chars += sum(len(term) for term in terms)\n",
        "                for term in terms:\n",
        "                    if doc_id not in index[term]:\n",
        "                        index[term].append(doc_id)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    return index, total_time, total_tokens, total_chars\n",
        "\n",
        "file_path = \"Text_Only_Ascii_Coll_NoSem\"\n",
        "index, total_time, total_tokens, total_chars = index_collection(file_path)\n",
        "print(f\"Temps d'indexation : {total_time}s\")\n",
        "print(f\"Nombre total de tokens : {total_tokens}\")\n",
        "print(f\"Nombre total de caractères : {total_chars}\")\n",
        "print(f\"Nombre de termes distincts : {len(index)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx0HnvWPN6gl",
        "outputId": "8ecf7faf-8a1a-4850-cc5c-9035d011c645"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temps d'indexation : 3217.2785229682922s\n",
            "Nombre total de tokens : 8871666\n",
            "Nombre total de caractères : 46134689\n",
            "Nombre de termes distincts : 163253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def remove_stop_words(index, stop_words_file):\n",
        "    with open(stop_words_file, 'r', encoding='utf-8') as file:\n",
        "        stop_words = set(word.strip() for word in file)\n",
        "    return {term: postings for term, postings in index.items() if term not in stop_words}\n",
        "\n",
        "def apply_stemming(index):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_index = defaultdict(list)\n",
        "    for term, postings in index.items():\n",
        "        stemmed_term = stemmer.stem(term)\n",
        "        stemmed_index[stemmed_term].extend(postings)\n",
        "    return stemmed_index\n",
        "\n",
        "stop_words_file = \"stop-words-english4.txt\"\n",
        "filtered_index = remove_stop_words(index, stop_words_file)\n",
        "print(f\"Index après suppression des mots vides : {len(filtered_index)} termes distincts.\")\n",
        "\n",
        "stemmed_index = apply_stemming(filtered_index)\n",
        "print(f\"Index après stemming : {len(stemmed_index)} termes distincts.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fIk1-62axFi",
        "outputId": "91be4a01-73c0-4633-cb45-da89b0a53880"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index après suppression des mots vides : 162641 termes distincts.\n",
            "Index après stemming : 129837 termes distincts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_smart_ltn(index, document_lengths):\n",
        "    weights = defaultdict(dict)\n",
        "    for term, postings in index.items():\n",
        "        df = len(postings)\n",
        "        idf = math.log10(len(document_lengths) / df) if df > 0 else 0\n",
        "        for doc_id in postings:\n",
        "            tf = index[term].count(doc_id)\n",
        "            weight = (1 + math.log10(tf)) * idf if tf > 0 else 0\n",
        "            weights[term][doc_id] = weight\n",
        "    return weights\n",
        "\n",
        "def compute_query_scores(query, weights, document_lengths):\n",
        "    query_terms = tokenize(query)\n",
        "    scores = defaultdict(float)\n",
        "    for term in query_terms:\n",
        "        if term in weights:\n",
        "            for doc_id, weight in weights[term].items():\n",
        "                scores[doc_id] += weight\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "document_lengths = {doc_id: 100 for doc_id in range(1, 10001)}\n",
        "query = \"web ranking scoring algorithm\"\n",
        "ltn_weights = compute_smart_ltn(stemmed_index, document_lengths)\n",
        "top_10 = compute_query_scores(query, ltn_weights, document_lengths)\n",
        "print(\"Top-10 documents avec SMART ltn :\", top_10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1B4WWN-a75j",
        "outputId": "a8f6252d-0e4a-46db-9911-c19ed80dca11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 documents avec SMART ltn : [(22952, 0.15069561188507175), (10803, 0.15069561188507175), (63837, 0.15069561188507175), (10822, 0.15069561188507175), (417, 0.11582793047608728), (5594, 0.11582793047608728), (7376, 0.11582793047608728), (14389, 0.11582793047608728), (14393, 0.11582793047608728), (14397, 0.11582793047608728)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2fY8EYOlEid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_smart_ltc(index, document_lengths):\n",
        "    weights = compute_smart_ltn(index, document_lengths)\n",
        "    for doc_id in document_lengths.keys():\n",
        "        norm = math.sqrt(sum((weights[term][doc_id] ** 2 for term in weights if doc_id in weights[term])))\n",
        "        for term in weights:\n",
        "            if doc_id in weights[term]:\n",
        "                weights[term][doc_id] /= norm\n",
        "    return weights\n",
        "\n",
        "ltc_weights = compute_smart_ltc(stemmed_index, document_lengths)\n",
        "top_10 = compute_query_scores(query, ltc_weights, document_lengths)\n",
        "print(\"Top-10 documents avec SMART ltc :\", top_10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbSeM6odfDGw",
        "outputId": "2451f2f3-f6ab-4492-b766-2f81ebde5eb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 documents avec SMART ltc : [(22952, 0.15069561188507175), (10803, 0.15069561188507175), (63837, 0.15069561188507175), (10822, 0.15069561188507175), (14389, 0.11582793047608728), (14393, 0.11582793047608728), (14397, 0.11582793047608728), (14406, 0.11582793047608728), (14778, 0.11582793047608728), (19344, 0.11582793047608728)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bm25(index, document_lengths, avg_doc_length, k1=1.2, b=0.75):\n",
        "    weights = defaultdict(dict)\n",
        "    for term, postings in index.items():\n",
        "        df = len(postings)  # Document frequency\n",
        "        idf = math.log10((len(document_lengths) - df + 0.5) / (df + 0.5) + 1)\n",
        "        for doc_id in postings:\n",
        "            if doc_id not in document_lengths:\n",
        "                document_lengths[doc_id] = 100  # Valeur par défaut\n",
        "            tf = index[term].count(doc_id)\n",
        "            doc_length = document_lengths[doc_id]\n",
        "            numerator = tf * (k1 + 1)\n",
        "            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))\n",
        "            weights[term][doc_id] = idf * (numerator / denominator)\n",
        "    return weights\n",
        "\n",
        "# Assurez-vous que document_lengths contient toutes les entrées\n",
        "max_doc_id = max(doc_id for term in index for doc_id in index[term])\n",
        "document_lengths = {doc_id: 100 for doc_id in range(1, max_doc_id + 1)}\n",
        "avg_doc_length = sum(document_lengths.values()) / len(document_lengths)\n",
        "\n",
        "bm25_weights = compute_bm25(stemmed_index, document_lengths, avg_doc_length)\n",
        "top_10 = compute_query_scores(query, bm25_weights, document_lengths)\n",
        "print(\"Top-10 documents avec BM25 :\", top_10)\n"
      ],
      "metadata": {
        "id": "8NlfTYuulFpm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}